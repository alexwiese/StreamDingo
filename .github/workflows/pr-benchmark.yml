name: PR Benchmark

on:
  pull_request:
    branches: [ main ]
    types: [opened, synchronize, reopened]
    paths:
      - 'src/**'
      - 'benchmarks/**'
      - 'tests/**'
      - '*.csproj'
      - '*.sln*'

permissions:
  contents: read
  pull-requests: write
  issues: write

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    if: ${{ !contains(github.event.pull_request.labels.*.name, 'skip-benchmark') }}
    
    steps:
    - name: Checkout PR branch
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.head.sha }}
        
    - name: Setup .NET
      uses: actions/setup-dotnet@v4
      with:
        dotnet-version: '9.0.x'
        
    - name: Restore dependencies
      run: dotnet restore
      
    - name: Build solution
      run: dotnet build --no-restore --configuration Release --verbosity normal
      
    - name: Run PR benchmarks
      id: pr_benchmark
      run: |
        cd benchmarks/StreamDingo.Benchmarks
        
        # Run benchmarks with very short job for CI speed
        timeout 300 dotnet run --configuration Release -- --job VeryShortRun --exporters json --filter "*" --memory || {
          echo "Benchmark run timed out or failed, but continuing with available results"
          exit 0
        }
        
        # Create results directory if it doesn't exist
        mkdir -p ../../benchmark-results/pr
        
        # Copy JSON results if they exist
        if ls BenchmarkDotNet.Artifacts/results/*.json 1> /dev/null 2>&1; then
          cp BenchmarkDotNet.Artifacts/results/*.json ../../benchmark-results/pr/
          echo "JSON results copied successfully"
        else
          echo "Warning: No JSON benchmark results found"
          # Create a dummy result file so the workflow doesn't fail
          echo '{"Benchmarks": []}' > ../../benchmark-results/pr/dummy-results.json
        fi
        
        # Store results as artifacts
        echo "pr_results_path=benchmark-results/pr" >> $GITHUB_OUTPUT
        
    - name: Upload PR benchmark artifacts
      uses: actions/upload-artifact@v4
      with:
        name: pr-benchmark-results
        path: benchmark-results/pr/
        retention-days: 7
        
    - name: Checkout main branch
      uses: actions/checkout@v4
      with:
        ref: main
        clean: false
        path: main-branch
        
    - name: Build main branch solution
      run: |
        cd main-branch
        dotnet restore
        dotnet build --no-restore --configuration Release --verbosity normal
        
    - name: Run main branch benchmarks
      id: main_benchmark
      run: |
        cd main-branch/benchmarks/StreamDingo.Benchmarks
        
        # Run benchmarks with very short job for CI speed  
        timeout 300 dotnet run --configuration Release -- --job VeryShortRun --exporters json --filter "*" --memory || {
          echo "Benchmark run timed out or failed, but continuing with available results"
          exit 0
        }
        
        # Create results directory if it doesn't exist
        mkdir -p ../../../benchmark-results/main
        
        # Copy JSON results if they exist
        if ls BenchmarkDotNet.Artifacts/results/*.json 1> /dev/null 2>&1; then
          cp BenchmarkDotNet.Artifacts/results/*.json ../../../benchmark-results/main/
          echo "JSON results copied successfully"
        else
          echo "Warning: No JSON benchmark results found"
          # Create a dummy result file so the workflow doesn't fail
          echo '{"Benchmarks": []}' > ../../../benchmark-results/main/dummy-results.json
        fi
        
    - name: Upload main benchmark artifacts
      uses: actions/upload-artifact@v4
      with:
        name: main-benchmark-results
        path: benchmark-results/main/
        retention-days: 7
        
    - name: Compare benchmark results
      id: compare
      run: |
        # Create comparison script
        cat > compare_benchmarks.py << 'EOF'
        import json
        import os
        import sys
        from pathlib import Path
        
        def load_benchmark_results(results_dir):
            """Load benchmark results from JSON files"""
            results = {}
            results_path = Path(results_dir)
            
            if not results_path.exists():
                return {}
                
            for json_file in results_path.glob("*.json"):
                try:
                    with open(json_file, 'r') as f:
                        data = json.load(f)
                        if 'Benchmarks' in data:
                            for benchmark in data['Benchmarks']:
                                key = f"{benchmark.get('Type', 'Unknown')}.{benchmark.get('Method', 'Unknown')}"
                                if benchmark.get('Parameters'):
                                    params = '_'.join(str(v) for v in benchmark['Parameters'].values())
                                    key += f"({params})"
                                
                                stats = benchmark.get('Statistics', {})
                                results[key] = {
                                    'mean': stats.get('Mean', 0),
                                    'median': stats.get('Median', 0),
                                    'stddev': stats.get('StandardDeviation', 0),
                                    'allocated': benchmark.get('Memory', {}).get('BytesAllocatedPerOperation', 0)
                                }
                except Exception as e:
                    print(f"Error reading {json_file}: {e}")
                    
            return results
        
        def format_time(nanoseconds):
            """Format time in appropriate units"""
            if nanoseconds < 1000:
                return f"{nanoseconds:.2f} ns"
            elif nanoseconds < 1_000_000:
                return f"{nanoseconds/1000:.2f} μs"
            elif nanoseconds < 1_000_000_000:
                return f"{nanoseconds/1_000_000:.2f} ms"
            else:
                return f"{nanoseconds/1_000_000_000:.2f} s"
        
        def format_bytes(bytes_val):
            """Format bytes in appropriate units"""
            if bytes_val == 0:
                return "0 B"
            elif bytes_val < 1024:
                return f"{bytes_val} B"
            elif bytes_val < 1024**2:
                return f"{bytes_val/1024:.2f} KB"
            elif bytes_val < 1024**3:
                return f"{bytes_val/1024**2:.2f} MB"
            else:
                return f"{bytes_val/1024**3:.2f} GB"
        
        def calculate_change_percentage(old_val, new_val):
            """Calculate percentage change"""
            if old_val == 0:
                return float('inf') if new_val > 0 else 0
            return ((new_val - old_val) / old_val) * 100
        
        def generate_markdown_report(main_results, pr_results):
            """Generate markdown comparison report"""
            
            if not main_results and not pr_results:
                return "No benchmark results found."
                
            if not main_results:
                return "No baseline results from main branch found."
                
            if not pr_results:
                return "No PR benchmark results found."
            
            report = []
            report.append("## 📊 Performance Benchmark Results")
            report.append("")
            report.append("This PR introduces the following performance changes:")
            report.append("")
            
            # Find common benchmarks
            common_benchmarks = set(main_results.keys()) & set(pr_results.keys())
            
            if not common_benchmarks:
                report.append("No common benchmarks found between main and PR branches.")
                return "\n".join(report)
            
            # Performance changes table
            report.append("| Benchmark | Main | PR | Change | Memory Main | Memory PR | Memory Change |")
            report.append("|-----------|------|----|---------|-----------|---------|-----------   |")
            
            significant_changes = []
            
            for benchmark in sorted(common_benchmarks):
                main_stat = main_results[benchmark]
                pr_stat = pr_results[benchmark]
                
                main_time = main_stat['mean']
                pr_time = pr_stat['mean']
                time_change = calculate_change_percentage(main_time, pr_time)
                
                main_memory = main_stat['allocated']
                pr_memory = pr_stat['allocated']
                memory_change = calculate_change_percentage(main_memory, pr_memory)
                
                # Format time change
                if time_change == float('inf'):
                    time_change_str = "NEW"
                elif abs(time_change) < 0.1:
                    time_change_str = "~"
                else:
                    sign = "🔴" if time_change > 5 else "🟡" if time_change > -5 else "🟢"
                    time_change_str = f"{sign} {time_change:+.1f}%"
                
                # Format memory change
                if memory_change == float('inf'):
                    memory_change_str = "NEW"
                elif abs(memory_change) < 0.1:
                    memory_change_str = "~"
                else:
                    sign = "🔴" if memory_change > 5 else "🟡" if memory_change > -5 else "🟢"
                    memory_change_str = f"{sign} {memory_change:+.1f}%"
                
                report.append(f"| {benchmark} | {format_time(main_time)} | {format_time(pr_time)} | {time_change_str} | {format_bytes(main_memory)} | {format_bytes(pr_memory)} | {memory_change_str} |")
                
                # Track significant changes
                if abs(time_change) > 10 or abs(memory_change) > 10:
                    change_type = "regression" if time_change > 10 or memory_change > 10 else "improvement"
                    significant_changes.append(f"- **{benchmark}**: {change_type}")
            
            if significant_changes:
                report.append("")
                report.append("### ⚡ Significant Changes")
                report.extend(significant_changes)
            
            report.append("")
            report.append("---")
            report.append("*🟢 = Improvement, 🟡 = Minor change, 🔴 = Regression*")
            
            return "\n".join(report)
        
        # Load results
        main_results = load_benchmark_results("benchmark-results/main")
        pr_results = load_benchmark_results("benchmark-results/pr")
        
        # Generate report
        markdown_report = generate_markdown_report(main_results, pr_results)
        
        # Save report
        with open("benchmark_report.md", "w") as f:
            f.write(markdown_report)
        
        print("Benchmark comparison completed!")
        print(f"Found {len(main_results)} main benchmarks and {len(pr_results)} PR benchmarks")
        EOF
        
        # Run comparison
        python3 compare_benchmarks.py
        
        # Set output
        echo "report_generated=true" >> $GITHUB_OUTPUT
        
    - name: Update PR description with benchmark results
      if: steps.compare.outputs.report_generated == 'true'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          // Read the benchmark report
          let benchmarkReport = '';
          try {
            benchmarkReport = fs.readFileSync('benchmark_report.md', 'utf8');
          } catch (error) {
            console.log('Could not read benchmark report:', error);
            return;
          }
          
          // Get current PR description
          const pr = await github.rest.pulls.get({
            owner: context.repo.owner,
            repo: context.repo.repo,
            pull_number: context.issue.number
          });
          
          let currentBody = pr.data.body || '';
          
          // Remove existing benchmark section
          const benchmarkSectionRegex = /<!--BENCHMARK_START-->.*?<!--BENCHMARK_END-->/s;
          currentBody = currentBody.replace(benchmarkSectionRegex, '').trim();
          
          // Add new benchmark section
          const benchmarkSection = `
          
          <!--BENCHMARK_START-->
          <details>
          <summary>📊 Performance Benchmark Results</summary>
          
          ${benchmarkReport}
          
          </details>
          <!--BENCHMARK_END-->`;
          
          const newBody = currentBody + benchmarkSection;
          
          // Update PR description
          await github.rest.pulls.update({
            owner: context.repo.owner,
            repo: context.repo.repo,
            pull_number: context.issue.number,
            body: newBody
          });
          
          console.log('PR description updated with benchmark results');
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          
    - name: Comment on performance regressions
      if: steps.compare.outputs.report_generated == 'true'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          // This would be enhanced to actually parse the results and detect regressions
          // For now, it's a placeholder for the functionality
          const report = fs.readFileSync('benchmark_report.md', 'utf8');
          
          if (report.includes('🔴')) {
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '⚠️ **Performance Warning**: This PR introduces performance regressions. Please review the benchmark results above.'
            });
          } else if (report.includes('🟢')) {
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '🚀 **Performance Improvement**: This PR includes performance improvements! Check out the benchmark results above.'
            });
          }