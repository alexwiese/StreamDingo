name: PR Benchmark

'on':
  pull_request:
    branches: [ main ]
    types: [opened, synchronize, reopened]
    paths:
      - 'src/**'
      - 'benchmarks/**'
      - 'tests/**'
      - '*.csproj'
      - '*.sln*'
  workflow_dispatch:
    inputs:
      pr_number:
        description: 'PR number to benchmark (leave empty to use current branch)'
        required: false
        type: string
      benchmark_filter:
        description: 'Benchmark filter (e.g., "*AppendEvents*")'
        required: false
        default: '*'
        type: string
      benchmark_mode:
        description: 'Benchmark mode for CI speed'
        required: false
        default: 'essential'
        type: choice
        options:
          - 'essential'
          - 'minimal'
          - 'fast'

permissions:
  contents: read
  pull-requests: write
  issues: write

jobs:
  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    if: ${{ !contains(github.event.pull_request.labels.*.name, 'skip-benchmark') }}

    steps:
    - name: Check if benchmarks are needed
      id: check_benchmarks
      run: |
        # Use minimal benchmarks for docs-only changes, full benchmarks for code changes
        if [[ "${{ contains(github.event.pull_request.labels.*.name, 'documentation') }}" == "true" ]] ||
           [[ "${{ github.event.pull_request.changed_files }}" =~ ^(docs/|README|CHANGELOG|\.md$) ]]; then
          echo "benchmark_mode=minimal" >> $GITHUB_OUTPUT
          echo "Using minimal benchmarks for documentation changes"
        else
          echo "benchmark_mode=${{ github.event.inputs.benchmark_mode || 'essential' }}" >> $GITHUB_OUTPUT
          echo "Using full benchmarks for code changes"
        fi
    - name: Checkout PR branch
      uses: actions/checkout@v4
      with:
        ref: ${{ github.event.pull_request.head.sha }}

    - name: Setup .NET
      uses: actions/setup-dotnet@v4
      with:
        dotnet-version: '9.0.x'

    - name: Cache .NET packages
      uses: actions/cache@v4
      with:
        path: ~/.nuget/packages
        key: ${{ runner.os }}-nuget-${{ hashFiles('**/packages.lock.json', '**/*.csproj') }}
        restore-keys: |
          ${{ runner.os }}-nuget-

    - name: Restore dependencies
      run: dotnet restore

    - name: Cache build artifacts
      uses: actions/cache@v4
      with:
        path: |
          **/bin/Release
          **/obj
        key: ${{ runner.os }}-build-${{ github.sha }}
        restore-keys: |
          ${{ runner.os }}-build-

    - name: Build solution
      run: dotnet build --no-restore --configuration Release --verbosity normal

    - name: Run PR benchmarks
      id: pr_benchmark
      run: |
        cd benchmarks/StreamDingo.Benchmarks

        # Use CI-optimized benchmarks for speed
        # Use dynamic mode based on change type detection
        BENCHMARK_MODE="${{ steps.check_benchmarks.outputs.benchmark_mode }}"
        BENCHMARK_FILTER="${{ github.event.inputs.benchmark_filter || '*' }}"

        echo "Running benchmarks in CI mode: $BENCHMARK_MODE with filter: $BENCHMARK_FILTER"

        # Run with shorter timeout for CI-optimized benchmarks
        timeout 120 dotnet run --configuration Release -- --ci-mode "$BENCHMARK_MODE" --exporters json --filter "$BENCHMARK_FILTER" || {
          echo "Benchmark run timed out or failed, but continuing with available results"
          exit 0
        }

        # Create results directory if it doesn't exist
        mkdir -p ../../benchmark-results/pr

        # Copy JSON results if they exist
        if ls BenchmarkDotNet.Artifacts/results/*.json 1> /dev/null 2>&1; then
          cp BenchmarkDotNet.Artifacts/results/*.json ../../benchmark-results/pr/
          echo "JSON results copied successfully"
        else
          echo "Warning: No JSON benchmark results found"
          # Create a dummy result file so the workflow doesn't fail
          echo '{"Benchmarks": []}' > ../../benchmark-results/pr/dummy-results.json
        fi

        # Store results as artifacts
        echo "pr_results_path=benchmark-results/pr" >> $GITHUB_OUTPUT

    - name: Upload PR benchmark artifacts
      uses: actions/upload-artifact@v4
      with:
        name: pr-benchmark-results
        path: benchmark-results/pr/
        retention-days: 7

    - name: Checkout main branch
      uses: actions/checkout@v4
      with:
        ref: main
        clean: false
        path: main-branch

    - name: Build main branch solution
      run: |
        cd main-branch
        # Use cache if available, otherwise restore
        dotnet restore
        dotnet build --no-restore --configuration Release --verbosity normal

    - name: Run main branch benchmarks
      id: main_benchmark
      run: |
        cd main-branch/benchmarks/StreamDingo.Benchmarks

        # Use same CI-optimized configuration as PR branch for fair comparison
        BENCHMARK_MODE="${{ steps.check_benchmarks.outputs.benchmark_mode }}"
        BENCHMARK_FILTER="${{ github.event.inputs.benchmark_filter || '*' }}"

        echo "Running main branch benchmarks in CI mode: $BENCHMARK_MODE with filter: $BENCHMARK_FILTER"

        # Run with shorter timeout for CI-optimized benchmarks
        timeout 120 dotnet run --configuration Release -- --ci-mode "$BENCHMARK_MODE" --exporters json --filter "$BENCHMARK_FILTER" || {
          echo "Benchmark run timed out or failed, but continuing with available results"
          exit 0
        }

        # Create results directory if it doesn't exist
        mkdir -p ../../../benchmark-results/main

        # Copy JSON results if they exist
        if ls BenchmarkDotNet.Artifacts/results/*.json 1> /dev/null 2>&1; then
          cp BenchmarkDotNet.Artifacts/results/*.json ../../../benchmark-results/main/
          echo "JSON results copied successfully"
        else
          echo "Warning: No JSON benchmark results found"
          # Create a dummy result file so the workflow doesn't fail
          echo '{"Benchmarks": []}' > ../../../benchmark-results/main/dummy-results.json
        fi

    - name: Upload main benchmark artifacts
      uses: actions/upload-artifact@v4
      with:
        name: main-benchmark-results
        path: benchmark-results/main/
        retention-days: 7

    - name: Compare benchmark results
      id: compare
      run: |
        # Create comparison script
        cat > compare_benchmarks.py << 'EOF'
        import json
        import os
        import sys
        from pathlib import Path

        def load_benchmark_results(results_dir):
            """Load benchmark results from JSON files"""
            results = {}
            results_path = Path(results_dir)
            
            print(f"Loading benchmark results from: {results_path}")
            if not results_path.exists():
                print(f"Results directory does not exist: {results_path}")
                return {}

            json_files = list(results_path.glob("*.json"))
            print(f"Found {len(json_files)} JSON files: {[f.name for f in json_files]}")

            for json_file in json_files:
                try:
                    with open(json_file, 'r') as f:
                        data = json.load(f)
                        if 'Benchmarks' in data:
                            for benchmark in data['Benchmarks']:
                                key = f"{benchmark.get('Type', 'Unknown')}.{benchmark.get('Method', 'Unknown')}"
                                if benchmark.get('Parameters'):
                                    # Handle both string and dictionary parameter formats
                                    params_val = benchmark['Parameters']
                                    if isinstance(params_val, dict):
                                        params = '_'.join(str(v) for v in params_val.values())
                                    else:
                                        # BenchmarkDotNet exports parameters as string (e.g., "eventCount=100")
                                        params = str(params_val).replace('=', '_').replace(' ', '_')
                                    key += f"({params})"

                                stats = benchmark.get('Statistics', {})
                                results[key] = {
                                    'mean': stats.get('Mean', 0),
                                    'median': stats.get('Median', 0),
                                    'stddev': stats.get('StandardDeviation', 0),
                                    'allocated': benchmark.get('Memory', {}).get('BytesAllocatedPerOperation', 0)
                                }
                except Exception as e:
                    print(f"Error reading {json_file}: {e}")
                    # Continue processing other files instead of failing completely
                    continue

            print(f"Successfully loaded {len(results)} benchmark results")
            for key in results.keys():
                print(f"  - {key}")
            return results

        def format_time(nanoseconds):
            """Format time in appropriate units"""
            if nanoseconds < 1000:
                return f"{nanoseconds:.2f} ns"
            elif nanoseconds < 1_000_000:
                return f"{nanoseconds/1000:.2f} Î¼s"
            elif nanoseconds < 1_000_000_000:
                return f"{nanoseconds/1_000_000:.2f} ms"
            else:
                return f"{nanoseconds/1_000_000_000:.2f} s"

        def format_bytes(bytes_val):
            """Format bytes in appropriate units"""
            if bytes_val == 0:
                return "0 B"
            elif bytes_val < 1024:
                return f"{bytes_val} B"
            elif bytes_val < 1024**2:
                return f"{bytes_val/1024:.2f} KB"
            elif bytes_val < 1024**3:
                return f"{bytes_val/1024**2:.2f} MB"
            else:
                return f"{bytes_val/1024**3:.2f} GB"

        def calculate_change_percentage(old_val, new_val):
            """Calculate percentage change"""
            if old_val == 0:
                return float('inf') if new_val > 0 else 0
            return ((new_val - old_val) / old_val) * 100

        def generate_markdown_report(main_results, pr_results):
            """Generate markdown comparison report"""

            if not main_results and not pr_results:
                return "No benchmark results found."

            if not main_results:
                return "No baseline results from main branch found."

            if not pr_results:
                return "No PR benchmark results found."

            report = []
            report.append("## ð Performance Benchmark Results")
            report.append("")
            report.append(f"ð **CI-Optimized Benchmarks**: Using fast configuration for quick PR feedback")
            report.append("")
            report.append("This PR introduces the following performance changes:")
            report.append("")

            # Find common benchmarks
            common_benchmarks = set(main_results.keys()) & set(pr_results.keys())

            if not common_benchmarks:
                report.append("No common benchmarks found between main and PR branches.")
                return "\n".join(report)

            # Calculate overall performance summary
            performance_improvements = 0
            performance_regressions = 0
            memory_improvements = 0
            memory_regressions = 0

            # Performance changes table
            report.append("| Benchmark | Main | PR | Change | Memory Main | Memory PR | Memory Change |")
            report.append("|-----------|------|----|---------|-----------|---------|-----------   |")

            significant_changes = []

            for benchmark in sorted(common_benchmarks):
                main_stat = main_results[benchmark]
                pr_stat = pr_results[benchmark]

                main_time = main_stat['mean']
                pr_time = pr_stat['mean']
                time_change = calculate_change_percentage(main_time, pr_time)

                main_memory = main_stat['allocated']
                pr_memory = pr_stat['allocated']
                memory_change = calculate_change_percentage(main_memory, pr_memory)

                # Track overall trends
                if time_change > 5:
                    performance_regressions += 1
                elif time_change < -5:
                    performance_improvements += 1

                if memory_change > 5:
                    memory_regressions += 1
                elif memory_change < -5:
                    memory_improvements += 1

                # Format time change
                if time_change == float('inf'):
                    time_change_str = "NEW"
                elif abs(time_change) < 0.1:
                    time_change_str = "~"
                else:
                    sign = "ð´" if time_change > 5 else "ð¡" if time_change > -5 else "ð¢"
                    time_change_str = f"{sign} {time_change:+.1f}%"

                # Format memory change
                if memory_change == float('inf'):
                    memory_change_str = "NEW"
                elif abs(memory_change) < 0.1:
                    memory_change_str = "~"
                else:
                    sign = "ð´" if memory_change > 5 else "ð¡" if memory_change > -5 else "ð¢"
                    memory_change_str = f"{sign} {memory_change:+.1f}%"

                report.append(f"| {benchmark} | {format_time(main_time)} | {format_time(pr_time)} | {time_change_str} | {format_bytes(main_memory)} | {format_bytes(pr_memory)} | {memory_change_str} |")

                # Track significant changes
                if abs(time_change) > 10 or abs(memory_change) > 10:
                    change_type = "regression" if time_change > 10 or memory_change > 10 else "improvement"
                    significant_changes.append(f"- **{benchmark}**: {change_type}")

            # Add performance summary
            report.append("")
            report.append("### ð Performance Summary")

            if performance_improvements > 0 or performance_regressions > 0:
                report.append(f"- **Performance**: {performance_improvements} improvements, {performance_regressions} regressions")
            if memory_improvements > 0 or memory_regressions > 0:
                report.append(f"- **Memory**: {memory_improvements} improvements, {memory_regressions} regressions")

            # Visual performance indicator
            if performance_regressions > performance_improvements:
                report.append("- **Overall Impact**: â ï¸ Performance decreased")
            elif performance_improvements > performance_regressions:
                report.append("- **Overall Impact**: â Performance improved")
            else:
                report.append("- **Overall Impact**: â¡ï¸ No significant performance change")

            # Add ASCII chart for visual representation
            if len(common_benchmarks) > 0:
                report.append("")
                report.append("### ð Performance Change Distribution")
                report.append("```")
                report.append("Performance Changes:")
                improvements_bar = "ð¢" * min(performance_improvements, 20)
                regressions_bar = "ð´" * min(performance_regressions, 20)
                neutral_bar = "âª" * min(len(common_benchmarks) - performance_improvements - performance_regressions, 20)
                report.append(f"Improvements: {improvements_bar} ({performance_improvements})")
                report.append(f"Regressions:  {regressions_bar} ({performance_regressions})")
                report.append(f"Neutral:      {neutral_bar} ({len(common_benchmarks) - performance_improvements - performance_regressions})")
                report.append("```")

            if significant_changes:
                report.append("")
                report.append("### â¡ Significant Changes")
                report.extend(significant_changes)

            report.append("")
            report.append("---")
            report.append("*ð¢ = Improvement, ð¡ = Minor change, ð´ = Regression*")

            return "\n".join(report)

        # Load results
        main_results = load_benchmark_results("benchmark-results/main")
        pr_results = load_benchmark_results("benchmark-results/pr")

        # Generate report
        markdown_report = generate_markdown_report(main_results, pr_results)

        # Save report
        with open("benchmark_report.md", "w") as f:
            f.write(markdown_report)

        print("Benchmark comparison completed!")
        print(f"Found {len(main_results)} main benchmarks and {len(pr_results)} PR benchmarks")
        EOF

        # Run comparison
        python3 compare_benchmarks.py

        # Set output
        echo "report_generated=true" >> $GITHUB_OUTPUT

    - name: Update PR description with benchmark results
      if: steps.compare.outputs.report_generated == 'true'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');

          // Read the benchmark report
          let benchmarkReport = '';
          try {
            benchmarkReport = fs.readFileSync('benchmark_report.md', 'utf8');
          } catch (error) {
            console.log('Could not read benchmark report:', error);
            return;
          }

          // Get current PR description
          const pr = await github.rest.pulls.get({
            owner: context.repo.owner,
            repo: context.repo.repo,
            pull_number: context.issue.number
          });

          let currentBody = pr.data.body || '';

          // Remove existing benchmark section
          const benchmarkSectionRegex = /<!--BENCHMARK_START-->.*?<!--BENCHMARK_END-->/s;
          currentBody = currentBody.replace(benchmarkSectionRegex, '').trim();

          // Add new benchmark section
          const benchmarkSection = `

          <!--BENCHMARK_START-->
          <details>
          <summary>ð Performance Benchmark Results</summary>

          ${benchmarkReport}

          </details>
          <!--BENCHMARK_END-->`;

          const newBody = currentBody + benchmarkSection;

          // Update PR description
          await github.rest.pulls.update({
            owner: context.repo.owner,
            repo: context.repo.repo,
            pull_number: context.issue.number,
            body: newBody
          });

          console.log('PR description updated with benchmark results');
      env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    - name: Comment on performance regressions
      if: steps.compare.outputs.report_generated == 'true'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');

          // This would be enhanced to actually parse the results and detect regressions
          // For now, it's a placeholder for the functionality
          const report = fs.readFileSync('benchmark_report.md', 'utf8');

          if (report.includes('ð´')) {
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'â ï¸ **Performance Warning**: This PR introduces performance regressions. Please review the benchmark results above.'
            });
          } else if (report.includes('ð¢')) {
            await github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'ð **Performance Improvement**: This PR includes performance improvements! Check out the benchmark results above.'
            });
          }

