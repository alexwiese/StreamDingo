<h1 id="benchmarks">Benchmarks</h1>
<p>StreamDingo uses <a href="https://benchmarkdotnet.org/">BenchmarkDotNet</a> to measure and track performance characteristics across different scenarios.</p>
<h2 id="running-benchmarks">Running Benchmarks</h2>
<h3 id="local-development">Local Development</h3>
<p>To run benchmarks locally:</p>
<pre><code class="language-bash">cd benchmarks/StreamDingo.Benchmarks
dotnet run --configuration Release
</code></pre>
<p>For specific benchmarks:</p>
<pre><code class="language-bash">dotnet run --configuration Release -- --filter &quot;*EventAppend*&quot;
</code></pre>
<h3 id="benchmark-categories">Benchmark Categories</h3>
<p>The benchmark suite covers:</p>
<ul>
<li><strong>Event Appending</strong>: Throughput of adding events to streams</li>
<li><strong>Event Replay</strong>: Speed of reconstructing state from events</li>
<li><strong>Hash Calculation</strong>: Performance of integrity verification</li>
<li><strong>Snapshot Creation</strong>: Memory and time costs of snapshots</li>
<li><strong>Memory Allocation</strong>: GC pressure and memory usage patterns</li>
</ul>
<h2 id="automated-performance-tracking">Automated Performance Tracking</h2>
<h3 id="pr-benchmarks">PR Benchmarks</h3>
<p>Every pull request automatically:</p>
<ol>
<li><strong>Runs baseline benchmarks</strong> on the main branch</li>
<li><strong>Runs PR benchmarks</strong> on the proposed changes</li>
<li><strong>Compares results</strong> and calculates performance deltas</li>
<li><strong>Updates PR description</strong> with collapsible performance results</li>
<li><strong>Comments on significant changes</strong> (&gt;10% regression/improvement)</li>
</ol>
<h3 id="benchmark-reports">Benchmark Reports</h3>
<p>Performance reports include:</p>
<pre><code class="language-markdown">## 游늵 Performance Benchmark Results

| Benchmark | Main | PR | Change | Memory Main | Memory PR | Memory Change |
|-----------|------|----|---------|-----------|---------|-----------   |
| AppendEvents(100) | 124.3 풮s | 118.2 풮s | 游릭 -4.9% | 2.1 KB | 2.0 KB | 游릭 -4.8% |
| ReplayEvents(1000) | 1.2 ms | 1.3 ms | 游댮 +8.3% | 15.2 KB | 16.1 KB | 游리 +5.9% |
</code></pre>
<p>Legend:</p>
<ul>
<li>游릭 = Performance improvement</li>
<li>游리 = Minor change (&lt;10%)</li>
<li>游댮 = Performance regression</li>
</ul>
<h3 id="historical-tracking">Historical Tracking</h3>
<p>Benchmark results are:</p>
<ul>
<li>Stored as artifacts for 30 days</li>
<li>Tracked in GitHub Issues for significant changes</li>
<li>Used to build performance trend reports</li>
</ul>
<h2 id="benchmark-infrastructure">Benchmark Infrastructure</h2>
<h3 id="benchmarkdotnet-configuration">BenchmarkDotNet Configuration</h3>
<pre><code class="language-csharp">[MemoryDiagnoser]
[Orderer(SummaryOrderPolicy.FastestToSlowest)]
[RankColumn]
public class EventSourcingBenchmarks
{
    [Benchmark]
    [Arguments(100, 1000, 10000)]
    public void AppendEvents(int eventCount) { /* ... */ }
}
</code></pre>
<h3 id="continuous-integration">Continuous Integration</h3>
<p>The benchmark workflow:</p>
<ol>
<li><strong>Triggers</strong>: On every PR to main branch</li>
<li><strong>Environment</strong>: Ubuntu Latest with .NET 9.0</li>
<li><strong>Execution</strong>: Short job profile for CI speed</li>
<li><strong>Output</strong>: JSON format for automated parsing</li>
<li><strong>Analysis</strong>: Python script for comparison and reporting</li>
</ol>
<h3 id="skipping-benchmarks">Skipping Benchmarks</h3>
<p>To skip benchmarks on a PR (for documentation-only changes):</p>
<pre><code>Add the label: skip-benchmark
</code></pre>
<h2 id="interpreting-results">Interpreting Results</h2>
<h3 id="understanding-metrics">Understanding Metrics</h3>
<ul>
<li><strong>Mean</strong>: Average execution time</li>
<li><strong>Median</strong>: 50th percentile execution time</li>
<li><strong>StdDev</strong>: Standard deviation (consistency indicator)</li>
<li><strong>Allocated</strong>: Memory allocated per operation</li>
</ul>
<h3 id="performance-thresholds">Performance Thresholds</h3>
<ul>
<li><strong>Green (Improvement)</strong>: &gt;5% faster or less memory</li>
<li><strong>Yellow (Neutral)</strong>: 췀5% change</li>
<li><strong>Red (Regression)</strong>: &gt;5% slower or more memory</li>
</ul>
<h3 id="when-to-be-concerned">When to Be Concerned</h3>
<p>Review carefully if:</p>
<ul>
<li>Core operations show &gt;10% regression</li>
<li>Memory usage increases significantly</li>
<li>New allocations appear in hot paths</li>
<li>Standard deviation increases (less consistent performance)</li>
</ul>
<h2 id="contributing-to-benchmarks">Contributing to Benchmarks</h2>
<p>When adding new features:</p>
<ol>
<li><strong>Add corresponding benchmarks</strong> for new operations</li>
<li><strong>Ensure benchmarks are realistic</strong> and representative</li>
<li><strong>Include both best-case and worst-case scenarios</strong></li>
<li><strong>Test with different data sizes</strong> using <code>[Arguments]</code></li>
<li><strong>Verify benchmarks pass</strong> before submitting PR</li>
</ol>
<h3 id="benchmark-best-practices">Benchmark Best Practices</h3>
<pre><code class="language-csharp">[GlobalSetup]
public void Setup()
{
    // Initialize test data once
    // Avoid setup work in benchmark methods
}

[Benchmark]
public void MyBenchmark()
{
    // Keep benchmark focused
    // Avoid external dependencies
    // Use realistic data sizes
}
</code></pre>
